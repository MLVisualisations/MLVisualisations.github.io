<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>ML Visualisations</title>
  <meta content="" name="descriptison">
  <meta content="" name="keywords">

  <script type="text/javascript" src="assets/vendor/MathJax/MathJax.js">
  MathJax.Hub.Config({
      extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
      jax: ["input/TeX","output/HTML-CSS"],
      tex2jax: {
          inlineMath: [['$','$'],["\\(","\\)"]],
          processEscapes: true,
      },
  });
</script>

  <!-- Favicons -->
  <link href="assets/img/favicon.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Poppins:300,400,500,600,700" rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/ionicons/css/ionicons.min.css" rel="stylesheet">
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="assets/vendor/owl.carousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">
</head>

<body>

  <!-- ======= Header/Navbar ======= -->

  <nav class="navbar navbar-default navbar-trans navbar-expand-lg fixed-top">
    <div class="container">

      <a class="navbar-brand text-brand" href="index.html">Visual<span class="color-b">ML</span></a>
    </div>
  </nav>
  <!-- End Header/Navbar -->

  <!-- ======= Intro Section ======= -->
  <div class="intro-single">
  </div><!-- End Intro Section -->

  <main id="main">

    <!-- ======= Blog Single ======= -->
    <section class="news-single nav-arrow-b">
      <div class="container">
        <div class="row">
          <div class="col-md-10 offset-md-1 col-lg-8 offset-lg-2">

            <div class="post-content color-text-a">
              <p class="post-intro">
                Neural Networks
              </p>

              <p>
                Most introductory posts on neural networks start by introducing
                a biological neuron and then go on to explain how we can represent
                a biological neuron by a step function. To simply the training
                procedure, we then approximate the step function by a smooth
                sigmoid or hyperbolic tan function. Here we are going to take a
                fairly different approach by looking at the geometry behind a
                single neuron. Then build up some intuition as to why we use
                layers of neurons and then stack these layers. Using these ideas,
                you will build up an understanding for the Universal Approximation
                Theorem and the relation between neural networks and decision trees.
              </p>

              <p>
                Anyhow, a neuron is just a function which takes multiple inputs,
                i.e., a vector and produces one output, a scalar. For simplicity
                of presentations, lets suppose our input is of dimension 3.
              </p>

              <p align="center">
                <img width="400" height="400" src="src/neural_network/img/fig1.png">
              </p>

              <p>
                The simplest possible function which turns a vector into a scalar
                is a weighted sum which can be represented by an inner product,
                $$f(x_1, x_2, x_3) = \begin{pmatrix} w_1 & w_2 & w_3 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} = w_1 x_1 + w_2 x_2 + w_3 x_3.$$
                To increase the flexibility of the function, we could also include
                an additional parameter known as the bias, so
                $$f(x_1, x_2, x_3) = \begin{pmatrix} w_1 & w_2 & w_3 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} + b = w_1 x_1 + w_2 x_2 + w_3 x_3 + b.$$
              </p>

              <p>
                To make our artifical neuron behave like a biological neuron, we apply
                a step function. So finally we have
                $$f(x_1, x_2, x_3) = \sigma \left(\begin{pmatrix} w_1 & w_2 & w_3 \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix} + b \right).$$
                Where $\sigma$ is the step function which looks like this:
              </p>

              <p align="center">
                <img width="600" height="200" src="src/neural_network/img/fig3.png">
              </p>

              <p>
                A single neuron with a step function actually just splits the input
                space into two using a hyperplane. Consider a two dimensional input
                (this is the easiest to visualise), the weights control the direction
                of the hyperplane.
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig4.png">
              </p>

              <p>
                And the bias controls the shift.
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig5.png">
              </p>

              <p>
                If we were using a neural network for a binary classification problem,
                then it is clear that a single neuron is very restrictive. It is
                natural to question, what if we have more than one neuron. But remember,
                a single neuron always produces a scalar, so two neurons next to
                each other actually produce a vector of dimension two. The output of
                a layer of n neurons is in fact a vector of dimension n.
              </p>

              <p align="center">
                <img width="400" height="400" src="src/neural_network/img/fig6.png">
              </p>

              <p>
                Two neurons equates to two hyperplanes where each hyperplane splits
                the regions into two. And so two neurons could lead to at most
                four regions. Whereas $n$ neurons leads to at most $2^n$ regions.
                The output is a binary vector where each entry relates to either
                the left and side or right hand side of its corresponding hyperplane.
                After this layer, any point in a single region is indistinguishable
                from any other point in that same region.
              </p>

              <p align="center">
                <img width="500" height="400" src="src/neural_network/img/fig7.png">
              </p>

              <p>
                In fact the layer of two neurons has mapped every point in the
                original two dimensional space to one of these 4 node. Each nodes
                corresponds to a corner of a $n$ dimensional hypercube (a square
                in our case).
              </p>

              <p align="center">
                <img width="500" height="400" src="src/neural_network/img/fig8.png">
              </p>

              <p>
                Well what if we fed the output of this layer into another single
                neuron?
              </p>

              <p align="center">
                <img width="300" height="400" src="src/neural_network/img/fig9.png">
              </p>

              <p>
                Well this neuron doesnt realise that the input space is one of four points,
                it just divides the whole space regardless. But now the output of
                this two layered neural network is quite interesting. We have put act
                the points mapping to $[0, 1]$ in one class and all the other points
                in another.
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig10.png">
              </p>

              <p>
                And so the neural network actually classifies this region.
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig11.png">
              </p>

              <p>
                So the first layer of the neural network just carves up regions in
                our space and the second layer groups these regions. Whatever classification
                problem you gave me, I would be able to, with enough neurons, carve up
                the space such that no region contained elements from different classes.
                And then my second layer could group all the regions belonging to
                a specific class. This is the premise of the universal Approximation
                theorem.
              </p>

              <p>
                Lets consider a simple classification problem.
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig12.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig13.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig14.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig15.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig16.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig17.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig18.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig19.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig20.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig21.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig22.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig23.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig24.png">
              </p>

              <p align="center">
                <img width="600" height="400" src="src/neural_network/img/fig25.png">
              </p>
            </div>
          </div>
       </div>
      </div>
    </section><!-- End Blog Single-->

  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <section class="section-footer">
    <div class="container">
      <div class="row">

      </div>
    </div>
  </section>

  <a href="#" class="back-to-top"><i class="fa fa-chevron-up"></i></a>
  <div id="preloader"></div>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/jquery/jquery.min.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/jquery.easing/jquery.easing.min.js"></script>
  <script src="assets/vendor/owl.carousel/owl.carousel.min.js"></script>
  <script src="assets/vendor/scrollreveal/scrollreveal.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

</body>

</html>
